{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5c7013",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-24T03:27:25.588234Z",
     "iopub.status.busy": "2025-09-24T03:27:25.587850Z",
     "iopub.status.idle": "2025-09-24T03:27:31.977215Z",
     "shell.execute_reply": "2025-09-24T03:27:31.976474Z"
    },
    "papermill": {
     "duration": 6.394671,
     "end_time": "2025-09-24T03:27:31.978870",
     "exception": false,
     "start_time": "2025-09-24T03:27:25.584199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b3e408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:27:31.984493Z",
     "iopub.status.busy": "2025-09-24T03:27:31.983621Z",
     "iopub.status.idle": "2025-09-24T03:27:31.991792Z",
     "shell.execute_reply": "2025-09-24T03:27:31.990960Z"
    },
    "papermill": {
     "duration": 0.01238,
     "end_time": "2025-09-24T03:27:31.993319",
     "exception": false,
     "start_time": "2025-09-24T03:27:31.980939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleECGDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for ECG CSV files\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_files, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_files: List of paths to CSV files\n",
    "            labels: List of corresponding labels\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.labels = labels\n",
    "        assert len(csv_files) == len(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load CSV (5000, 12) and transpose to (12, 5000) for Conv1d\n",
    "        ecg_data = np.loadtxt(self.csv_files[idx], dtype=np.float32, delimiter=',')\n",
    "        ecg_data = (ecg_data - ecg_data.mean()) / (ecg_data.std() + 1e-8)  # Normalize\n",
    "        ecg_tensor = torch.from_numpy(ecg_data.T)  # (12, 5000)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return ecg_tensor, label\n",
    "\n",
    "def create_simple_dataloaders(csv_files, labels, train_ratio=0.8, batch_size=16):\n",
    "    \"\"\"Create basic train/val dataloaders\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data\n",
    "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "        csv_files, labels, train_size=train_ratio, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SimpleECGDataset(train_files, train_labels)\n",
    "    val_dataset = SimpleECGDataset(val_files, val_labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b1933",
   "metadata": {
    "papermill": {
     "duration": 0.001496,
     "end_time": "2025-09-24T03:27:31.996871",
     "exception": false,
     "start_time": "2025-09-24T03:27:31.995375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41be04d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:27:32.001670Z",
     "iopub.status.busy": "2025-09-24T03:27:32.001323Z",
     "iopub.status.idle": "2025-09-24T03:27:32.018881Z",
     "shell.execute_reply": "2025-09-24T03:27:32.017930Z"
    },
    "papermill": {
     "duration": 0.021887,
     "end_time": "2025-09-24T03:27:32.020341",
     "exception": false,
     "start_time": "2025-09-24T03:27:31.998454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ECGDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for ECG CSV files\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_files, labels, normalize=True, augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_files: List of paths to CSV files\n",
    "            labels: List of corresponding labels (same length as csv_files)\n",
    "            normalize: Whether to normalize ECG signals\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.labels = labels\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        \n",
    "        assert len(csv_files) == len(labels), \"Number of files must match number of labels\"\n",
    "        \n",
    "        # Compute normalization statistics if needed\n",
    "        if normalize:\n",
    "            self._compute_normalization_stats()\n",
    "    \n",
    "    def _compute_normalization_stats(self):\n",
    "        \"\"\"Compute mean and std across all ECG files for normalization\"\"\"\n",
    "        print(\"Computing normalization statistics...\")\n",
    "        all_data = []\n",
    "        \n",
    "        # Sample a subset for stats if dataset is very large\n",
    "        sample_indices = np.random.choice(len(self.csv_files), \n",
    "                                        min(100, len(self.csv_files)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            try:\n",
    "                data = np.loadtxt(self.csv_files[idx], dtype=float, delimiter=',')\n",
    "                if data.shape == (5000, 12):\n",
    "                    all_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {self.csv_files[idx]}: {e}\")\n",
    "        \n",
    "        if all_data:\n",
    "            all_data = np.concatenate(all_data, axis=0)  # Shape: (N*5000, 12)\n",
    "            self.mean = np.mean(all_data, axis=0, keepdims=True)  # Shape: (1, 12)\n",
    "            self.std = np.std(all_data, axis=0, keepdims=True)   # Shape: (1, 12)\n",
    "            self.std = np.where(self.std == 0, 1, self.std)  # Avoid division by zero\n",
    "            print(f\"Normalization stats computed from {len(all_data)} samples\")\n",
    "        else:\n",
    "            print(\"Warning: Could not compute normalization stats, using defaults\")\n",
    "            self.mean = np.zeros((1, 12))\n",
    "            self.std = np.ones((1, 12))\n",
    "    \n",
    "    def _normalize_ecg(self, data):\n",
    "        \"\"\"Normalize ECG data using precomputed stats\"\"\"\n",
    "        return (data - self.mean) / self.std\n",
    "    \n",
    "    def _augment_ecg(self, data):\n",
    "        \"\"\"Apply data augmentation to ECG signal\"\"\"\n",
    "        # Random noise addition\n",
    "        if np.random.random() < 0.3:\n",
    "            noise = np.random.normal(0, 0.02, data.shape)\n",
    "            data = data + noise\n",
    "        \n",
    "        # Random scaling\n",
    "        if np.random.random() < 0.3:\n",
    "            scale = np.random.uniform(0.9, 1.1)\n",
    "            data = data * scale\n",
    "        \n",
    "        # Random baseline shift per lead\n",
    "        if np.random.random() < 0.3:\n",
    "            baseline_shift = np.random.normal(0, 0.05, (1, 12))\n",
    "            data = data + baseline_shift\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load ECG data\n",
    "            ecg_data = np.loadtxt(self.csv_files[idx], dtype=np.float32, delimiter=',')\n",
    "            \n",
    "            # Validate shape\n",
    "            if ecg_data.shape != (5000, 12):\n",
    "                raise ValueError(f\"Expected shape (5000, 12), got {ecg_data.shape}\")\n",
    "            \n",
    "            # Normalize\n",
    "            if self.normalize:\n",
    "                ecg_data = self._normalize_ecg(ecg_data)\n",
    "            \n",
    "            # Augment (only during training)\n",
    "            if self.augment:\n",
    "                ecg_data = self._augment_ecg(ecg_data)\n",
    "            \n",
    "            # Convert to tensor and transpose to (12, 5000) for Conv1d\n",
    "            ecg_tensor = torch.from_numpy(ecg_data.T)  # Shape: (12, 5000)\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "            return ecg_tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.csv_files[idx]}: {e}\")\n",
    "            # Return a zero tensor and label if file is corrupted\n",
    "            return torch.zeros(12, 5000, dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "\n",
    "def create_ecg_dataloaders(csv_files, labels, train_ratio=0.8, batch_size=16, \n",
    "                          num_workers=4, normalize=True, augment_train=True):\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders from ECG CSV files\n",
    "    \n",
    "    Args:\n",
    "        csv_files: List of CSV file paths\n",
    "        labels: List of corresponding labels\n",
    "        train_ratio: Fraction of data to use for training\n",
    "        batch_size: Batch size for dataloaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        normalize: Whether to normalize ECG signals\n",
    "        augment_train: Whether to augment training data\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, class_counts\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for easier handling\n",
    "    csv_files = np.array(csv_files)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Stratified split to maintain class balance\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "        csv_files, labels, \n",
    "        train_size=train_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: {len(train_files)} train, {len(val_files)} validation\")\n",
    "    \n",
    "    # Count classes\n",
    "    from collections import Counter\n",
    "    class_counts = Counter(labels)\n",
    "    train_class_counts = Counter(train_labels)\n",
    "    val_class_counts = Counter(val_labels)\n",
    "    \n",
    "    print(\"Overall class distribution:\", dict(class_counts))\n",
    "    print(\"Train class distribution:\", dict(train_class_counts))\n",
    "    print(\"Val class distribution:\", dict(val_class_counts))\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ECGDataset(train_files.tolist(), train_labels.tolist(), \n",
    "                              normalize=normalize, augment=augment_train)\n",
    "    val_dataset = ECGDataset(val_files.tolist(), val_labels.tolist(), \n",
    "                            normalize=normalize, augment=False)\n",
    "    \n",
    "    # Copy normalization stats to validation dataset\n",
    "    if normalize:\n",
    "        val_dataset.mean = train_dataset.mean\n",
    "        val_dataset.std = train_dataset.std\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size*2, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, class_counts"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.336198,
   "end_time": "2025-09-24T03:27:33.443515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-24T03:27:21.107317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
